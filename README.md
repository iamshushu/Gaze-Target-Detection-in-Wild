# Gaze-Target-Detection-in-Wild

<aside>

ğŸ’» **Three-stage method to simulate the human gaze inference behavior in 3D space**

    Stage 1) Estimate a 3D gaze orientation from the head

    Stage 2) Develop a Dual Attention Module (DAM)

    Stage 3) Use the generated dual attention as guidance to perform two sub-tasks: 

    Stage 3-1) Identifying whether the gaze target is inside or out of the image 

    Stage 3-2) Locating the target if inside

    
    
    
</aside>

> **ë…¼ë¬¸ì—ì„œ ë°œì „ì‹œí‚¨ ë¶€ë¶„ !!**
> 


**ğŸ’¡ Gaze target detection aims to infer where people in a scene are looking**


**ğŸ’¡ Person in scene is looking (â†’ people in a scene are looking : ìµœì¢… ëª©í‘œ )**


**ğŸ’¡ The Common Gaze Point of Human Observer**


# Manual (Following model 1ï¸âƒ£ â†’ 2ï¸âƒ£ â†’ 3ï¸âƒ£)


## 1ï¸âƒ£ **Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer**

> MiDaS github link
> 

[https://github.com/isl-org/MiDaS](https://github.com/isl-org/MiDaS)

### MiDaS Manual

### <Setup>

1. Terminal Command (Clone a remote Git repository locally)
    
    ```powershell
    git clone https://github.com/isl-org/MiDaS.git
    ```
    
2. Pick one or more models and download the corresponding weights to theÂ `weights`Â folder
    
    ```
    Download model : [dpt_beit_large_512](https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_512.pt) 
    -> For highest quality
    ```
    
3. Set up dependencies :
    - Use environment.yaml
        
        ([https://github.com/islorg/MiDaS/blob/master/environment.yaml](https://github.com/isl-org/MiDaS/blob/master/environment.yaml))
        
        ```bash
        # environment.yaml
        name: midas-py310
        channels:
          - pytorch
          - defaults
        dependencies:
          - nvidia::cudatoolkit=11.7
          - python=3.10.8
          - pytorch::pytorch=1.13.0
          - torchvision=0.14.0
          - pip=22.3.1
          - numpy=1.23.4
          - pip:
            - opencv-python==4.6.0.66
            - imutils==0.5.4
            - timm==0.6.12
            - einops==0.6.0
        ```
        
    
    ```
    conda env create -f environment.yaml
    conda activate midas-py310
    ```
    

### <Usage>

1. Place one or more input images in the folderÂ `input`.
    
    (+ input folderë„ ë„£ì„ ìˆ˜ ìˆìŒ)
    
    ```
    **Input Dataset 1: VideoAttentionTarget**
    Download the VideoAttentionTarget dataset from https://www.dropbox.com/s/8ep3y1hd74wdjy5/videoattentiontarget.zip?dl=0
    
    **~~Input Dataset 2: GazeFollow~~**
    
    (Source : https://github.com/ejcgt/attention-target-detection)
    ```
    
2. Run the model with
    
    ```python
    python run.py --model_type <model_type> --input_path input --output_path output
    ```
    
    whereÂ `<model_type>`Â is chosen fromÂ [dpt_beit_large_512](https://github.com/isl-org/MiDaS#model_type)
    
3. The resulting depth maps are written to theÂ `output`Â folder.
    
    â†’ Get depth map images  ~~and pfm file~~ 
    

## 2ï¸âƒ£ Gaze360: Physically Unconstrained Gaze Estimation in the Wild Dataset

> Gaze360 github link
> 

[https://github.com/erkil1452/gaze360](https://github.com/erkil1452/gaze360)

- **Details ( Data, Structure )**
    - **Data**
        
        ```
        Gaze360 Dataset more information at http://gaze360.csail.mit.edu.
        1. Collected in indoor and outdoor environments; 
        	 A wide range of head poses and distances between subjects and cameras.
        2. This repository provides already processed txt files with the split for training the Gaze360 model. The txt contains the following information:
        		Row 1: Image path
        		Row 2-4: Gaze vector
        ```
        
    
    - **Structure (Original)**
        
        ```bash
        ${PROJECT}
        â”œâ”€â”€ code
        â”‚	â”œâ”€â”€ data_loader.py
        â”‚	â”œâ”€â”€ model.py
        â”‚	â”œâ”€â”€ resnet.py
        â”‚	â”œâ”€â”€ run.py
        â”‚	â”œâ”€â”€ test.txt
        â”‚	â”œâ”€â”€ train.txt
        â”‚	â””â”€â”€ validation.txt
        â””â”€â”€ dataset
        ```
        
    
    - Structure (Edit)
        
        ```bash
        ${PROJECT}
        â”œâ”€â”€ detectron2 (í´ë”)
        â”œâ”€â”€ output
        â”‚	â””â”€â”€ *.csv
        â”œâ”€â”€ demo.py
        â”œâ”€â”€ frame_to_video.py
        â”œâ”€â”€ gaze360_model.pth.tar (pretrained model)
        â”œâ”€â”€ gaze360.py
        â”œâ”€â”€ model.py
        â””â”€â”€ resnet.py
        ```
        
- ê¸°ì¡´ code ì—ì„œ ë³€ê²½ëœ ë¶€ë¶„
    
    â†’ pretrained ëœ modelì„ ì‚¬ìš©í•˜ë¯€ë¡œ, ë¶ˆí•„ìš”í•œ íŒŒì¼ì€ ì‚¬ìš© x
    
    â†’ output/csv ë¶€ë¶„ì€ demo.pyì—ì„œ ë‚˜ì˜¨ output(.csv)ì„ ì €ì¥í•˜ëŠ” folder
    
    â†’ [demo.py](http://demo.py) ëŠ” ì•„ë˜ <Usage> ë¶€ë¶„ì—ì„œ ì„¤ëª…í•¨, ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” ë¶€ë¶„
    
    â†’ frame_to_video.pyëŠ” frameì„ videoë¡œ ë§Œë“œëŠ” ê²½ìš°ì— ì‚¬ìš©, outputì„ videoë¡œ ë³´ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©
    
    â†’gaze360.py ëŠ” ì•„ë˜ <Setup> ë¶€ë¶„ì—ì„œ ì„¤ëª…í•¨, ëª¨ë¸ì„ test í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ (ì°¸ê³ ìš©)
    
    â†’ ë‚˜ë¨¸ì§€ëŠ” ë™ì¼ ..
    

### Gaze360 Manual

### <Setup>

- Requirements
    
    ```bash
    PyTorch 1.1.0 (but it is likely to work on previous version of PyTorch as well.)
    ```
    
1. Terminal Command (Clone a remote Git repository locally)
    
    ```powershell
    git clone https://github.com/erkil1452/gaze360.git
    ```
    
2. Installation
    
    ```bash
    # ê°€ìƒí™˜ê²½ ìƒì„±
    conda create -n gaze360-py37 python=3.7
    conda activate gaze360-py37
    
    # Install DensePose as package
    pip install git+https://github.com/facebookresearch/detectron2@main#subdirectory=projects/DensePose
    
    ######################
    # Fix lot of imports #
    ######################
    pip install imgaug==0.2.6
    pip install opencv-python==4.1.2.30
    # lucid ì´ìš©í•˜ê¸° ìœ„í•´ tensorflow ì„¤ì¹˜ 
    # (lucid is not currently supporting tensorflow 2)
    pip install tensorflow==1.15
    pip install tensorflow-gpu==1.15
    
    pip install lucid==0.2.3
    pip install moviepy
    
    # Download Detectron2 source.
    git clone https://github.com/facebookresearch/detectron2.git
    
    # Check whether the DensePose installation was successful:
    # !python detectron2/tests/test_model_analysis.py
    ```
    
3. Download Trained Model Weights from 
    
    [](https://gaze360.csail.mit.edu/files/gaze360_model.pth.tar)
    
4. Download video(video.mp4) ~~and run DensePose on it~~    
    
    (ì´ 4ë²ˆ ë¶€ë¶„ì€ Gaze360 ëª¨ë¸ì„ ê°€ì¥ **ê°„ë‹¨í•˜ê²Œ test**í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì„¤ëª…, 
    
    **ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œëŠ” ì•„ë˜ì˜ <Useage>** ë³´ê¸°!)
    
    > Extract the frames of the video in a temporary folder:
    > 
    1. Make the folder 
        
        `mkdir content`
        
        `cd content`
        
        `mkdir temp`
        
        `cd ..`
        
        â†’ ìµœì¢… : â€˜root directory/content/tempâ€™  folder ìƒì„±
        
        â†’í˜„ì¬ directiory : root directory 
        
    2. Download the video
        
        `wget http://gaze360.csail.mit.edu/files/video.mp4`
        
    3. Extract the frames  (output frames path : â€˜root directory/content/tempâ€™ )
        
        `ffmpeg -i video.mp4 ./content/temp/frame%04d.jpg` 
        
    4. test code ëª… : **gaze360.py**
        
        ```python
        python gaze360.py
        ```
       
    ---
    
    - Run DensePose Part (ì‚¬ìš©x, ì°¸ê³ ìš©)
        1. Detect the pose of all the people in the video
            
            `rm -r ./content/DensePose/DensePoseData/infer_out/*`
            
        2. Download model
            
            ```bash
            cd detectron2/projects/DensePose && wget [https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_101_FPN_s1x/165712084/model_final_c6ab63.pkl](https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_101_FPN_s1x/165712084/model_final_c6ab63.pkl)
            ```
            
        3. Test model
            
            ```bash
            #cd detectron2/projects/DensePose
            python apply_net.py show configs/densepose_rcnn_R_101_FPN_s1x.yaml model_final_c6ab63.pkl /content/temp/frame0001.jpg dp_contour,bbox --output image_densepose_contour.png
            ```
            
        4. Run DensePose on the video frames
            1.  **root directory** 
                
                â†’ ****ì‚¬ìš©ìì— ë”°ë¼ ë‹¬ë¼ì§, ë³€ê²½ í•„ìš”**
                
            
            ```bash
            mkdir DensePoseData
            #cd detectron2/projects/DensePose 
            python apply_net.py \
              dump \
              configs/densepose_rcnn_R_101_FPN_s1x.yaml model_final_c6ab63.pkl \
              **<root directory>**/content/temp \
              -v \
              --output <root directory>**/content/DensePoseData/data.pkl
            ```
            
        5. Run code
            
            ê·¸ ì „ì— í˜„ì¬ directory í™•ì¸ !
            
            root directoryì—ì„œ demo code ì‹¤í–‰
            
     ---   

    
    
### Usage

1. Run [demo.py](http://demo.py)
    
    ì‚¬ìš©ìì— ë”°ë¼ ë‹¤ë¥¸ pathë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë³€ê²½ í•„ìš”
    
    ```
    python demo.py --source_path <source_path> --output_csv_save_path <output_csv_save_path>
    ```
    
    - source_path : input(image) ì´ë¯¸ì§€ê°€ ë“¤ì–´ìˆëŠ” folder path, ìœ„ì˜ MiDaS input folderì™€ ë™ì¼
    - output_csv_save_path : output(csv)ì„ ì €ì¥í•  folder path
    
    - `input type` : image folder
        - VideoAttentionTargetì˜ ê²½ìš°, image folder ì˜ êµ¬ì„±
            
            ex) ../data/videoattentiontarget/images/All in the Family/1023_1249/00001029.jpg
            
        - input ê³¼ ê´€ë ¨ëœ ë³€ìˆ˜ ì„¤ëª…
            - img_path_list
                
                : image folder ë‚´ì— ìˆëŠ” .jpg íŒŒì¼ë“¤ì˜ pathë¥¼ ì €ì¥í•œ list
                
            - image_list
                
                : img_path_list ë‚´ .jpg íŒŒì¼ì„ ì½ì€ ê±¸ ì €ì¥í•œ list
                
    
    - `output type` : csv íŒŒì¼
        - VideoAttentionTargetì˜ ê²½ìš°, output csv í˜•íƒœ
            
            ex) ../output/csv/output/All in the Family/1023_1249.csv
            
        - output csv ë‚´ìš©
            
            ```
            # frame ë‚´ì˜ ì‚¬ëŒë“¤ì˜ ì •ë³´ ì €ì¥
            # ê°™ì€ frame ë‚´ì— ìˆëŠ” ì‚¬ëŒì´ë©´ path ëŠ” ë™ì¼
            # frame ë‚´ì— ì—¬ëŸ¬ ì‚¬ëŒì´ ìˆëŠ” ê²½ìš°, head_x_min, head_y_min,head_x_max, head_y_max, gaze_x, gaze_y, gaze_z ëŠ” ëª¨ë‘ list í˜•íƒœë¡œ ì €ì¥
            [path, head_x_min, head_y_min,head_x_max, head_y_max, 
            gaze_x, gaze_y, gaze_z]
            ```
            

### ê¸°ì¡´ Modelì—ì„œ ë‹¬ë¼ì§„ ë¶€ë¶„ ì„¤ëª…

1. DensePose ëŒ€ì‹  <Detectron2>ì˜ object detection ì‚¬ìš©
    
    `input type` : image 
    
    `output type` : image ë‚´ ì‚¬ëŒë“¤ì˜ bounding Boxes ì¢Œí‘œ (x1, y1, x2, y2)
    
    ì´ë•Œ, ì‚¬ëŒì˜ body bounding box ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ëŒì˜ head bounding box ì¢Œí‘œ ì¶”ì¶œ
    
    â†’ ì´ ë¶€ë¶„ì€ ê°œì„  í•„ìš” !! ì„ì˜ì˜ ë¹„ìœ¨ëŒ€ë¡œ ìë¦„
    
    â†’ ì–¼êµ´ ì˜ì—­ì„ ì˜ ì¡ì•„ë‚´ì§€ ëª»í•˜ëŠ” ê²½ìš° ì¶”ì •í•˜ëŠ” 3D gaze directionì´ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
    
    â†’ Detectron2 ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë”ë¼ë„, ì‚¬ëŒì˜ ì–¼êµ´ ì˜ì—­ì˜ bounding box ë§Œ ì˜ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤ë©´,   ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥!!
    
2. ì¶”ì¶œí•œ ì‚¬ëŒì˜ head bounding box ì¢Œí‘œë¡œ cropí•œ head ì´ë¯¸ì§€ë¥¼ Gaze360ì˜ pretrained modelì— ëŒ€ì…í›„ 3D gaze directionì„ ì¶”ì •

## 3ï¸âƒ£ Dual Attention Guided Gaze Target Detection in the Wild

> DAM github link
> 

[https://github.com/Crystal2333/DAM](https://github.com/Crystal2333/DAM)

- Details ( Structure ) & Code
    
    
    - Structure ( Original )
        
        ```bash
        ${PROJECT}
        â”œâ”€â”€ dataset.py
        â”œâ”€â”€ gaze_model.py
        â”œâ”€â”€ model.py
        â”œâ”€â”€ resnet_scene.py
        â”œâ”€â”€ train_gazefollow.py
        â”œâ”€â”€ train_videoatttarget.py
        â””â”€â”€ util.py
        ```
        
    
    - Structure ( Edit )
        
        ```bash
        ${PROJECT}
        â”œâ”€â”€ data
        â”œâ”€â”€â””â”€â”€ videoattentiontarget
        â”œâ”€â”€  â”œâ”€â”€ annotations
        â”œâ”€â”€  â””â”€â”€ images
        â”œâ”€â”€ DAM_model_epoch50.pth
        â”œâ”€â”€ dataset.py
        â”œâ”€â”€ gaze_model.py
        â”œâ”€â”€ inference_edit.py
        â”œâ”€â”€ inference_mean.py
        â”œâ”€â”€ model_demo.py
        â”œâ”€â”€ model_train.py
        â”œâ”€â”€ model.py
        â”œâ”€â”€ resnet_scene.py
        â”œâ”€â”€ train_DAM_model_epoch50.py
        â””â”€â”€ util.py
        ```
        
    - ê¸°ì¡´ code ì—ì„œ ë³€ê²½ëœ ë¶€ë¶„ (# [ Original ] ê³¼ # [ Edit ] ë¶€ë¶„ )
        - data ëŠ” videoattentiontarget data
            
            â†’ train_DAM_model_epoch50.py ì—ì„œ modelì„ train í•  ë•Œ ì‚¬ìš©í•˜ëŠ” input data
            
            - annotation
                
                : frame ì´ë¦„, ê·¸ frame ë‚´ ì‚¬ëŒë“¤ì˜ head bounding box ì¢Œí‘œì™€ gaze target ì˜ ì¢Œí‘œê°€ .txt í˜•íƒœë¡œ ì œê³µ  (ex) s00.txt s01.txt)
                
                : frame.jpg head_bbox_x1, head_bbox_y1, head_bbox_x2, head_bbox_y2, target_x, target_y
                
                : gaze target ì´ ì—†ëŠ” ê²½ìš°, target_x, target_y ëŠ” ê°ê° -1, -1 ë¡œ ì œê³µ
                
            - images
        - DAM_model_epoch50.pth ëŠ” train_DAM_model_epoch50.py ì—ì„œ train í•œ ëª¨ë¸ì„ ì €ì¥í•œ ê²ƒ
        
### DAM Manual

### <Setup>

ìœ„ ëª¨ë¸ë“¤ì˜ setup í™˜ê²½ê³¼ ë™ì¼

> ì‚¬ìš©í•  Dataset
> 

Download the VideoAttentionTarget dataset from 

[https://www.dropbox.com/s/8ep3y1hd74wdjy5/videoattentiontarget.zip?dl=0](https://www.dropbox.com/s/8ep3y1hd74wdjy5/videoattentiontarget.zip?dl=0)

```
[VideoAttentionTarget dataset]
1. Image
- 1,331 video clips collected from various sources on YouTube
**********2. Annotation**********
- 164,541 frame-level head bounding boxes
- 109,574 in-frame gaze targets
- 54,967 out-of-frame gaze indicators
```

- ì´í›„ì— ì¶”ê°€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Datasets : **GazeFollow dataset, Gaze360**

### Usage

1. Pretrained modelì´ ì—†ìœ¼ë¯€ë¡œ train í•„ìš”
    - trainì— ì‚¬ìš©í•  dataset : VideoAttentionTarget
    - trainí•  ë•Œ ì‚¬ìš©í•  code : train_videoatttarget.py
    
    *ì‚¬ìš©ìì— ë”°ë¼ ë‹¤ë¥¸ pathë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë³€ê²½ í•„ìš”
    
    ```python
    data_dir = '/data/videoattentiontarget/images'
    depth_dir = '/data/videoattentiontarget/depthmap'
    train_annotation_dir = '/data/videoattentiontarget_new/annotations/train'
    test_annotation_dir = '/data/videoattentiontarget_new/annotations/test'
    ```
    
    > data_dir : VideoAttentionTarget dataset ì¤‘ imageì˜ path
    > 
    
    > depth_dir : VideoAttention Target imageë¥¼ ì´ìš©í•˜ì—¬ êµ¬í•œ depthmapì˜ path
    * ìœ„ì˜ MiDaSë¥¼ ì´ìš©í•˜ì—¬ êµ¬í•œ depthmapì´ë¯€ë¡œ MiDaS output pathì™€ ë™ì¼í•´ì•¼ í•¨ !!
    > 
    
    > train_annotation_dir : VideoAttentionTarget dataset ì¤‘ train annotationì˜ path
    > 
    
    > test_annotation_dir : VideoAttentionTarget dataset ì¤‘ test annotationì˜ path
    > 
    
    - output
        
        root folder ë‚´ì— model ì €ì¥
        
         DAM_model.pth
        

2-1 . Run inference_edit.py

- trained model ì´ìš©
    
    * ì‚¬ìš©ìì— ë”°ë¼ ë‹¤ë¥¸ pathë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë³€ê²½ í•„ìš”
    
    ```
    python inference_edit.py --annotation_dir <annotation_dir> --data_dir <data_dir> --depth_dir <depth_dir>
    ```
    
- `input type`
    - annotation_dir : gaze360 ì—ì„œ êµ¬í•œ csv fileì´ ì €ì¥ëœ path
    - data_dir : image folderì˜ path (MiDas gaze360ì—ì„œ ì‚¬ìš©í•œ input imageì˜ pathì™€ ë™ì¼)
    - depth_dir : MiDaSì—ì„œ êµ¬í•œ depth map imageê°€ ì €ì¥ëœ path
- `output type`
    - **ì˜ˆì¸¡ëœ gaze targetì„ visualizeí•œ image  â†’ backbone ì´ì „ì˜ dual attention image**
    

2-2 . Run inference_mean.py

- trained model ì´ìš©
    
    *ì‚¬ìš©ìì— ë”°ë¼ ë‹¤ë¥¸ pathë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë³€ê²½ í•„ìš”
    
    ```
    python inference_mean.py --annotation_dir <annotation_dir> --data_dir <data_dir> --depth_dir <depth_dir>
    ```
    
- `input type`
    - annotation_dir : **gaze360** ì—ì„œ êµ¬í•œ csv fileì´ ì €ì¥ëœ path
    - data_dir : image folderì˜ path (MiDas, gaze360ì—ì„œ ì‚¬ìš©í•œ input imageì˜ pathì™€ ë™ì¼)
    - depth_dir : **MiDaS**ì—ì„œ êµ¬í•œ depth map imageê°€ ì €ì¥ëœ path
- `output type`
    - ì˜ˆì¸¡ëœ gaze targetì„ visualizeí•œ imageâ†’ backbone ì´í›„ì˜ Output heatmap
